{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq57bw4AC3yk"
      },
      "source": [
        "###Ensemble methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS93sRrvb1n4"
      },
      "source": [
        "**Max voting**\n",
        "* The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
        "\n",
        "**Averaging**\n",
        "* Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
        "\n",
        "**Weighted average**\n",
        "* This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csq0MiLjcHaZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cIIO6EdcIBN"
      },
      "source": [
        "**Bagging**\n",
        "* The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. \n",
        "* Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. \n",
        "* Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). \n",
        "\n",
        "**Random forest**\n",
        "* Random Forest is another ensemble machine learning algorithm that follows the bagging technique. The base estimators in random forest are decision trees. Random forest randomly selects a set of features and rows which are used to decide the best split at each node of the decision tree.\n",
        "* Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction\n",
        "\n",
        "**Boosting**\n",
        "* If a data point is incorrectly predicted by the first model, and then the next (probably all models), will combining the predictions provide better results? Such situations are taken care of by boosting.\n",
        "* Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruao_tE7CvY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7cb6e0-fa98-4e24-bcf0-8e32f97394c0"
      },
      "source": [
        "# Bagging using multiple kinds of models\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
        "print(dataframe.head(5))\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7\n",
        "\n",
        "Dtc = SVC() # 76% \n",
        "#Dtc = GaussianNB() # 75%\n",
        "#Dtc = KNeighborsClassifier() # 73#\n",
        "#Dtc = DecisionTreeClassifier() # 76%\n",
        "\n",
        "model = BaggingClassifier(base_estimator=Dtc, n_estimators=100, random_state=seed, max_samples=300)\n",
        "model.fit(X,Y)\n",
        "print(model.score(X,Y))\n",
        "\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=20) # change cv and obeserve different outputs\n",
        "\n",
        "print(results)\n",
        "\n",
        "print(results.mean())\n",
        "print(results.min())\n",
        "print(results.max())\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n",
            "0.7591145833333334\n",
            "[0.74358974 0.76923077 0.71794872 0.71794872 0.71794872 0.66666667\n",
            " 0.76923077 0.58974359 0.71052632 0.76315789 0.78947368 0.78947368\n",
            " 0.68421053 0.78947368 0.73684211 0.84210526 0.78947368 0.73684211\n",
            " 0.78947368 0.78947368]\n",
            "0.7451417004048582\n",
            "0.5897435897435898\n",
            "0.8421052631578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wjdB95_Ee24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58719153-8ded-4ecf-e59f-ddb6b3f9b89d"
      },
      "source": [
        "# Random forest\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
        "\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, max_features=3)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=5)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7670061964179611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLMAsf9vE3yH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77aa612b-d477-429b-9805-4a8ea7dc17f0"
      },
      "source": [
        "# Boosting\n",
        "# AdaBoost - short for Adaptive Boosting.  \n",
        "# AdaBoost is adaptive in the sense that subsequent weak learners are \n",
        "# tweaked in favor of those instances misclassified by previous classifiers. \n",
        "# DecisionTreeClasifier is used as the default estimator\n",
        "\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
        "\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=123)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=5)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7617604617604619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe0bYVqAFHeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1ebdb8-5979-4742-e026-0a9ade7903c4"
      },
      "source": [
        "# Voting Ensemble for Classification\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
        "\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7\n",
        "\n",
        "# create the sub models\n",
        "estimators = []\n",
        "model1 = LogisticRegression()\n",
        "estimators.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "#model2 = RandomForestClassifier()\n",
        "estimators.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "estimators.append(('svm', model3))\n",
        "# create the ensemble model\n",
        "ensemble = VotingClassifier(estimators, weights=[3,2,3])\n",
        "results = model_selection.cross_val_score(ensemble, X, Y, cv=5)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7838978015448604\n"
          ]
        }
      ]
    }
  ]
}